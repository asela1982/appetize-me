{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .appName(\"chef\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import csv of preparation and rating\n",
    "df = spark.read.csv('data/heroku_kp3hp48w.recipes.csv',header=True,sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6674"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+----------------+\n",
      "|               title|         preparation|rating|makeitagainscore|\n",
      "+--------------------+--------------------+------+----------------+\n",
      "|Pasta with Lentil...|1. In a large pot...|     3|             75%|\n",
      "|Key Lime Pie with...|Combine huckleber...|     4|            100%|\n",
      "|       Old Fashioned|In old-fashioned ...|   3.5|            100%|\n",
      "|       Apple Galette|Blend flour and s...|   3.5|            100%|\n",
      "|Lemon-Spice Bread...|Preheat oven to 3...|     4|            100%|\n",
      "|    Soft Egg Ravioli|Mix all ingredien...|     4|            100%|\n",
      "|Mac and Cheese wi...|Prepare barbecue ...|     3|             78%|\n",
      "|Salt-Roasted Port...|Mix first 7 ingre...|     3|             70%|\n",
      "|Handmade Pasta wi...|Place 2 cups flou...|     4|            100%|\n",
      "|Pan-Seared Sea Sc...|Place wine and sh...|     3|             70%|\n",
      "|Chorizo-Filled Dates|Cook chorizo in h...|   3.5|            100%|\n",
      "|         Shabu-Shabu|Arrange steak on ...|     3|             90%|\n",
      "|Key Lime Cheeseca...|Preheat oven to 3...|     4|            100%|\n",
      "|English Muffin To...|Put oven rack in ...|   1.5|              8%|\n",
      "|           Succotash|Cook bacon in a 1...|   3.5|             93%|\n",
      "|Mexican Street Co...|Spread grilled co...|   3.5|             90%|\n",
      "|    Roasted Tomatoes|Put oven rack in ...|     3|             64%|\n",
      "|Corn and Tomato G...|Arrange tomato sl...|   3.5|             89%|\n",
      "|Baked Eggs with C...|Put oven rack in ...|   3.5|            100%|\n",
      "|Spaghetti with Br...|Cook pasta accord...|   3.5|            100%|\n",
      "+--------------------+--------------------+------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df.dropna(how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a length column to be used as a future feature\n",
    "from pyspark.sql.functions import length\n",
    "data = df.withColumn('length', length(df['title']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+----------------+------+\n",
      "|               title|         preparation|rating|makeitagainscore|length|\n",
      "+--------------------+--------------------+------+----------------+------+\n",
      "|Pasta with Lentil...|1. In a large pot...|     3|             75%|    27|\n",
      "|Key Lime Pie with...|Combine huckleber...|     4|            100%|    62|\n",
      "|       Old Fashioned|In old-fashioned ...|   3.5|            100%|    13|\n",
      "|       Apple Galette|Blend flour and s...|   3.5|            100%|    13|\n",
      "|Lemon-Spice Bread...|Preheat oven to 3...|     4|            100%|    46|\n",
      "|    Soft Egg Ravioli|Mix all ingredien...|     4|            100%|    16|\n",
      "|Mac and Cheese wi...|Prepare barbecue ...|     3|             78%|    40|\n",
      "|Salt-Roasted Port...|Mix first 7 ingre...|     3|             70%|    24|\n",
      "|Handmade Pasta wi...|Place 2 cups flou...|     4|            100%|    57|\n",
      "|Pan-Seared Sea Sc...|Place wine and sh...|     3|             70%|    65|\n",
      "|Chorizo-Filled Dates|Cook chorizo in h...|   3.5|            100%|    20|\n",
      "|         Shabu-Shabu|Arrange steak on ...|     3|             90%|    11|\n",
      "|Key Lime Cheeseca...|Preheat oven to 3...|     4|            100%|    53|\n",
      "|English Muffin To...|Put oven rack in ...|   1.5|              8%|    21|\n",
      "|           Succotash|Cook bacon in a 1...|   3.5|             93%|     9|\n",
      "|Mexican Street Co...|Spread grilled co...|   3.5|             90%|    27|\n",
      "|    Roasted Tomatoes|Put oven rack in ...|     3|             64%|    16|\n",
      "|Corn and Tomato G...|Arrange tomato sl...|   3.5|             89%|    22|\n",
      "|Baked Eggs with C...|Put oven rack in ...|   3.5|            100%|    29|\n",
      "|Spaghetti with Br...|Cook pasta accord...|   3.5|            100%|    39|\n",
      "+--------------------+--------------------+------+----------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6662"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DecimalType\n",
    "data = data.withColumn(\"rating_numeric\", data[\"rating\"].cast(DecimalType(precision=10, scale=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+----------------+------+--------------+\n",
      "|               title|         preparation|rating|makeitagainscore|length|rating_numeric|\n",
      "+--------------------+--------------------+------+----------------+------+--------------+\n",
      "|Pasta with Lentil...|1. In a large pot...|     3|             75%|    27|           3.0|\n",
      "|Key Lime Pie with...|Combine huckleber...|     4|            100%|    62|           4.0|\n",
      "|       Old Fashioned|In old-fashioned ...|   3.5|            100%|    13|           3.5|\n",
      "|       Apple Galette|Blend flour and s...|   3.5|            100%|    13|           3.5|\n",
      "|Lemon-Spice Bread...|Preheat oven to 3...|     4|            100%|    46|           4.0|\n",
      "|    Soft Egg Ravioli|Mix all ingredien...|     4|            100%|    16|           4.0|\n",
      "|Mac and Cheese wi...|Prepare barbecue ...|     3|             78%|    40|           3.0|\n",
      "|Salt-Roasted Port...|Mix first 7 ingre...|     3|             70%|    24|           3.0|\n",
      "|Handmade Pasta wi...|Place 2 cups flou...|     4|            100%|    57|           4.0|\n",
      "|Pan-Seared Sea Sc...|Place wine and sh...|     3|             70%|    65|           3.0|\n",
      "|Chorizo-Filled Dates|Cook chorizo in h...|   3.5|            100%|    20|           3.5|\n",
      "|         Shabu-Shabu|Arrange steak on ...|     3|             90%|    11|           3.0|\n",
      "|Key Lime Cheeseca...|Preheat oven to 3...|     4|            100%|    53|           4.0|\n",
      "|English Muffin To...|Put oven rack in ...|   1.5|              8%|    21|           1.5|\n",
      "|           Succotash|Cook bacon in a 1...|   3.5|             93%|     9|           3.5|\n",
      "|Mexican Street Co...|Spread grilled co...|   3.5|             90%|    27|           3.5|\n",
      "|    Roasted Tomatoes|Put oven rack in ...|     3|             64%|    16|           3.0|\n",
      "|Corn and Tomato G...|Arrange tomato sl...|   3.5|             89%|    22|           3.5|\n",
      "|Baked Eggs with C...|Put oven rack in ...|   3.5|            100%|    29|           3.5|\n",
      "|Spaghetti with Br...|Cook pasta accord...|   3.5|            100%|    39|           3.5|\n",
      "+--------------------+--------------------+------+----------------+------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "\n",
    "def classify_rating(rating):\n",
    "    if rating >= 4:\n",
    "        return 'good'\n",
    "    else:\n",
    "        return 'bad'\n",
    "\n",
    "classify_rating = udf(classify_rating, StringType())\n",
    "data = data.withColumn('class', classify_rating(data['rating_numeric']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+----------------+------+--------------+-----+\n",
      "|               title|         preparation|rating|makeitagainscore|length|rating_numeric|class|\n",
      "+--------------------+--------------------+------+----------------+------+--------------+-----+\n",
      "|Pasta with Lentil...|1. In a large pot...|     3|             75%|    27|           3.0|  bad|\n",
      "|Key Lime Pie with...|Combine huckleber...|     4|            100%|    62|           4.0| good|\n",
      "|       Old Fashioned|In old-fashioned ...|   3.5|            100%|    13|           3.5|  bad|\n",
      "|       Apple Galette|Blend flour and s...|   3.5|            100%|    13|           3.5|  bad|\n",
      "|Lemon-Spice Bread...|Preheat oven to 3...|     4|            100%|    46|           4.0| good|\n",
      "|    Soft Egg Ravioli|Mix all ingredien...|     4|            100%|    16|           4.0| good|\n",
      "|Mac and Cheese wi...|Prepare barbecue ...|     3|             78%|    40|           3.0|  bad|\n",
      "|Salt-Roasted Port...|Mix first 7 ingre...|     3|             70%|    24|           3.0|  bad|\n",
      "|Handmade Pasta wi...|Place 2 cups flou...|     4|            100%|    57|           4.0| good|\n",
      "|Pan-Seared Sea Sc...|Place wine and sh...|     3|             70%|    65|           3.0|  bad|\n",
      "|Chorizo-Filled Dates|Cook chorizo in h...|   3.5|            100%|    20|           3.5|  bad|\n",
      "|         Shabu-Shabu|Arrange steak on ...|     3|             90%|    11|           3.0|  bad|\n",
      "|Key Lime Cheeseca...|Preheat oven to 3...|     4|            100%|    53|           4.0| good|\n",
      "|English Muffin To...|Put oven rack in ...|   1.5|              8%|    21|           1.5|  bad|\n",
      "|           Succotash|Cook bacon in a 1...|   3.5|             93%|     9|           3.5|  bad|\n",
      "|Mexican Street Co...|Spread grilled co...|   3.5|             90%|    27|           3.5|  bad|\n",
      "|    Roasted Tomatoes|Put oven rack in ...|     3|             64%|    16|           3.0|  bad|\n",
      "|Corn and Tomato G...|Arrange tomato sl...|   3.5|             89%|    22|           3.5|  bad|\n",
      "|Baked Eggs with C...|Put oven rack in ...|   3.5|            100%|    29|           3.5|  bad|\n",
      "|Spaghetti with Br...|Cook pasta accord...|   3.5|            100%|    39|           3.5|  bad|\n",
      "+--------------------+--------------------+------+----------------+------+--------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|class|count|\n",
      "+-----+-----+\n",
      "|  bad| 5464|\n",
      "| good| 1198|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.groupBy('class').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline model accuracy is 82%\n",
    "\n",
    "#### A baseline result is the simplest possible prediction. Classification: If you have a classification problem, you can select the class that has the most observations and use that class as the result for all predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, StringIndexer, NGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create all the features to the data set\n",
    "class_to_num = StringIndexer(inputCol='class',outputCol='label')\n",
    "tokenizer = Tokenizer(inputCol=\"title\", outputCol=\"token_text\")\n",
    "stopremove = StopWordsRemover(inputCol='token_text',outputCol='stop_tokens')\n",
    "# ngram = NGram(n=2, inputCol=\"token_text\", outputCol=\"ngrams\")\n",
    "hashingTF = HashingTF(inputCol=\"stop_tokens\", outputCol='hash_token')\n",
    "idf = IDF(inputCol='hash_token', outputCol='idf_token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vector\n",
    "\n",
    "# Create feature vectors\n",
    "clean_up = VectorAssembler(inputCols=['idf_token', 'length'], outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a and run a data processing Pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "data_prep_pipeline = Pipeline(\n",
    "    stages=[class_to_num, tokenizer, stopremove, hashingTF, idf, clean_up])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform the pipeline\n",
    "cleaner = data_prep_pipeline.fit(data)\n",
    "cleaned = cleaner.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['title',\n",
       " 'preparation',\n",
       " 'rating',\n",
       " 'makeitagainscore',\n",
       " 'length',\n",
       " 'rating_numeric',\n",
       " 'class',\n",
       " 'label',\n",
       " 'token_text',\n",
       " 'stop_tokens',\n",
       " 'hash_token',\n",
       " 'idf_token',\n",
       " 'features']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(262145,[68874,13...|\n",
      "|  1.0|(262145,[13778,14...|\n",
      "|  0.0|(262145,[4200,247...|\n",
      "|  0.0|(262145,[98869,20...|\n",
      "|  1.0|(262145,[67305,70...|\n",
      "|  1.0|(262145,[12531,12...|\n",
      "|  0.0|(262145,[32310,65...|\n",
      "|  0.0|(262145,[111817,2...|\n",
      "|  1.0|(262145,[14894,15...|\n",
      "|  0.0|(262145,[23181,43...|\n",
      "|  0.0|(262145,[181360,2...|\n",
      "|  0.0|(262145,[240559,2...|\n",
      "|  1.0|(262145,[12826,50...|\n",
      "|  0.0|(262145,[126861,1...|\n",
      "|  0.0|(262145,[216711,2...|\n",
      "|  0.0|(262145,[42141,94...|\n",
      "|  0.0|(262145,[90825,11...|\n",
      "|  0.0|(262145,[42235,94...|\n",
      "|  0.0|(262145,[30121,65...|\n",
      "|  0.0|(262145,[6113,169...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleaned.select(['label', 'features']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Break data down into a training set and a testing set\n",
    "(train, test) = cleaned.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "\n",
    "# Create a Naive Bayes model and fit training data\n",
    "nb = NaiveBayes(smoothing=1.0, modelType='multinomial')\n",
    "nb_predictor = nb.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tranform the model with the testing data\n",
    "test_results = nb_predictor.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-----+----------+\n",
      "|rating|class|label|prediction|\n",
      "+------+-----+-----+----------+\n",
      "|     3|  bad|  0.0|       0.0|\n",
      "|   3.5|  bad|  0.0|       0.0|\n",
      "|     4| good|  1.0|       0.0|\n",
      "|     3|  bad|  0.0|       0.0|\n",
      "|   3.5|  bad|  0.0|       0.0|\n",
      "|     3|  bad|  0.0|       0.0|\n",
      "|   3.5|  bad|  0.0|       0.0|\n",
      "|   3.5|  bad|  0.0|       0.0|\n",
      "|     3|  bad|  0.0|       0.0|\n",
      "|   3.5|  bad|  0.0|       0.0|\n",
      "|   3.5|  bad|  0.0|       0.0|\n",
      "|   3.5|  bad|  0.0|       0.0|\n",
      "|     4| good|  1.0|       0.0|\n",
      "|   3.5|  bad|  0.0|       0.0|\n",
      "|     4| good|  1.0|       0.0|\n",
      "|     4| good|  1.0|       0.0|\n",
      "|     3|  bad|  0.0|       0.0|\n",
      "|     3|  bad|  0.0|       0.0|\n",
      "|   3.5|  bad|  0.0|       0.0|\n",
      "|   3.5|  bad|  0.0|       0.0|\n",
      "+------+-----+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_results.select('rating','class','label','prediction').show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model at predicting reviews was: 0.7482345312483297\n"
     ]
    }
   ],
   "source": [
    "#Use the Class Evaluator for a cleaner description\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "acc_eval = MulticlassClassificationEvaluator()\n",
    "acc = acc_eval.evaluate(test_results)\n",
    "print(f\"Accuracy of model at predicting reviews was: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
